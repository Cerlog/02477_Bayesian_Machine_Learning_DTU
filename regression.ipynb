{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearRegression(object):\n",
    "    \n",
    "    def __init__(self, Phi, y, alpha=1., beta=1.):\n",
    "        \n",
    "        # store data and hyperparameters\n",
    "        self.Phi, self.y = Phi, y\n",
    "        self.N, self.D = Phi.shape\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        \n",
    "        # compute posterior distribution\n",
    "        self.m, self.S = self.compute_posterior(alpha, beta)\n",
    "        self.log_marginal_likelihood = self.compute_marginal_likelihood(alpha, beta)\n",
    "\n",
    "        # perform sanity check of shapes/dimensions\n",
    "        self.check_dimensions()\n",
    "\n",
    "    def check_dimensions(self):\n",
    "        D = self.D\n",
    "        assert self.m.shape == (D, 1), f\"Wrong shape for posterior mean.\\nFor D = {D}, the shape of the posterior mean must be ({D}, 1), but the actual shape is ({self.m.shape})\"\n",
    "        assert self.S.shape == (D, D), f\"Wrong shape for posterior covariance.\\nFor D = {D}, the shape of the posterior mean must be ({D}, {D}), , but the actual shape is ({self.S.shape})\"\n",
    "        # assert self.log_marginal_likelihood.shape == (), f\"Wrong shape for log_marginal_likelihood.\\nThe shape of must be (), but the actual shape is ({self.log_marginal_likelihood.shape})\"\n",
    "\n",
    "    def compute_posterior(self, alpha, beta):\n",
    "        \"\"\" computes the posterior N(w|m, S) and return m, S.\n",
    "            Shape of m and S must be (D, 1) and (D, D), respectively  \"\"\"\n",
    "        \n",
    "        #############################################\n",
    "        # Insert your solution here\n",
    "        #############################################\n",
    "        \n",
    "        # compute prior and posterior precision \n",
    "        inv_S0 = alpha*np.identity(self.D)\n",
    "        A = inv_S0 + beta*(self.Phi.T@self.Phi)\n",
    "        \n",
    "        # compute mean and covariance \n",
    "        m = beta*np.linalg.solve(A, self.Phi.T)@self.y   # eq. (2) above\n",
    "        S = np.linalg.inv(A)                             # eq. (1) above\n",
    "        \n",
    "        #############################################\n",
    "        # End of solution\n",
    "        #############################################\n",
    "        return m, S\n",
    "      \n",
    "    def generate_prior_samples(self, num_samples):\n",
    "        \"\"\" generate samples from the prior  \"\"\"\n",
    "        return multivariate_normal.rvs(np.zeros(len(self.m)), (1/self.alpha)*np.identity(len(self.m)), size=num_samples)\n",
    "    \n",
    "    def generate_posterior_samples(self, num_samples):\n",
    "        \"\"\" generate samples from the posterior  \"\"\"\n",
    "        return multivariate_normal.rvs(self.m.ravel(), self.S, size=num_samples)\n",
    "    \n",
    "    def predict_f(self, Phi):\n",
    "        \"\"\" computes posterior mean (mu_f) and variance (var_f) of f(phi(x)) for each row in Phi-matrix.\n",
    "            If Phi is a [N, D]-matrix, then the shapes of both mu_f and var_f must be (N,)\n",
    "            The function returns (mu_f, var_f)\n",
    "        \"\"\"\n",
    "        mu_f = (Phi@self.m).ravel()   \n",
    "        var_f = np.diag(Phi@self.S@Phi.T)   \n",
    "        \n",
    "        # check dimensions before returning values\n",
    "        assert mu_f.shape == (Phi.shape[0],), \"Shape of mu_f seems wrong. Check your implementation\"\n",
    "        assert var_f.shape == (Phi.shape[0],), \"Shape of var_f seems wrong. Check your implementation\"\n",
    "        return mu_f, var_f\n",
    "        \n",
    "    def predict_y(self, Phi):\n",
    "        \"\"\" returns posterior predictive mean (mu_y) and variance (var_y) of y = f(phi(x)) + e for each row in Phi-matrix.\n",
    "            If Phi is a [N, D]-matrix, then the shapes of both mu_y and var_y must be (N,).\n",
    "            The function returns (mu_y, var_y)\n",
    "        \"\"\"\n",
    "        mu_f, var_f = self.predict_f(Phi)\n",
    "        mu_y = mu_f                  \n",
    "        var_y = var_f + 1/self.beta  \n",
    "\n",
    "        # check dimensions before returning values\n",
    "        assert mu_y.shape == (Phi.shape[0],), \"Shape of mu_y seems wrong. Check your implementation\"\n",
    "        assert var_y.shape == (Phi.shape[0],), \"Shape of var_y seems wrong. Check your implementation\"\n",
    "        return mu_y, var_y\n",
    "        \n",
    "    \n",
    "    def compute_marginal_likelihood(self, alpha, beta):\n",
    "        \"\"\" computes and returns log marginal likelihood p(y|alpha, beta) \"\"\"\n",
    "        inv_S0 = alpha*np.identity(self.D)\n",
    "        A = inv_S0 + beta*(self.Phi.T@self.Phi)\n",
    "        m = beta*np.linalg.solve(A, self.Phi.T)@self.y   # (eq. 3.53 in Bishop)\n",
    "        S = np.linalg.inv(A)                             # (eq. 3.54 in Bishop)\n",
    "        Em = beta/2*np.sum((self.y - self.Phi@m)**2) + alpha/2*np.sum(m**2)\n",
    "        return self.D/2*np.log(alpha) + self.N/2*np.log(beta) - Em - 0.5*np.linalg.slogdet(A)[1] - self.N/2*np.log(2*np.pi)\n",
    "         \n",
    "\n",
    "    def optimize_hyperparameters(self):\n",
    "        # optimizes hyperparameters using marginal likelihood\n",
    "        theta0 = np.array((np.log(self.alpha), np.log(self.beta)))\n",
    "        def negative_marginal_likelihood(theta):\n",
    "            alpha, beta = np.exp(theta[0]), np.exp(theta[1])\n",
    "            return -self.compute_marginal_likelihood(alpha, beta)\n",
    "\n",
    "        result = minimize(value_and_grad(negative_marginal_likelihood), theta0, jac=True)\n",
    "\n",
    "        # store new hyperparameters and recompute posterior\n",
    "        theta_opt = result.x\n",
    "        self.alpha, self.beta = np.exp(theta_opt[0]), np.exp(theta_opt[1])\n",
    "        self.m, self.S = self.compute_posterior(self.alpha, self.beta)\n",
    "        self.log_marginal_likelihood = self.compute_marginal_likelihood(self.alpha, self.beta)\n",
    "\n",
    "# sanity check of implementation\n",
    "model = BayesianLinearRegression(0.5*np.ones((2,2)), 2*np.ones((2, 1)), alpha=0.5, beta=0.5)       \n",
    "assert np.allclose(model.m, np.array([1, 1])), \"Something seems to be wrong with your implementation of the posterior mean. Please check your implementation.\" \n",
    "assert np.allclose(model.S, np.array([[1.5, -0.5], [-0.5, 1.5]])), \"Something seems to be wrong with your implementation of the posterior covariance. Please check your implementation.\" \n",
    "\n",
    "# fit model to toy dataset\n",
    "Phi_train = design_matrix(xtrain)\n",
    "model = BayesianLinearRegression(Phi_train, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1./(1 + np.exp(-x))\n",
    "log_npdf = lambda x, m, v: -(x-m)**2/(2*v) - 0.5*np.log(2*np.pi*v)\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, x, y, N, sigma2_alpha=1., sigma2_beta=1.):\n",
    "        # data\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.N = N\n",
    "\n",
    "        # hyperparameters\n",
    "        self.sigma2_alpha = sigma2_alpha\n",
    "        self.sigma2_beta = sigma2_beta\n",
    "\n",
    "    def f(self, x, alpha, beta):\n",
    "        \"\"\" implements eq. (3). Output must have the same shape as x \"\"\"\n",
    "        return alpha + beta*x \n",
    "        \n",
    "    def theta(self, x, alpha, beta):\n",
    "        \"\"\" implements eq. (2). Output must have the same shape as x \"\"\"\n",
    "        return sigmoid(self.f(x, alpha, beta)) \n",
    "\n",
    "    def log_prior(self, alpha, beta):\n",
    "        \"\"\" implements log. of eq. (8). Output must have the same shape as alpha and beta \"\"\"\n",
    "        return log_npdf(alpha, 0, self.sigma2_alpha) + log_npdf(beta, 0, self.sigma2_beta) \n",
    "\n",
    "    def log_likelihood(self, alpha, beta):\n",
    "        \"\"\" implements log. of eq. (5). Output must have the same shape as alpha and beta \"\"\"\n",
    "        theta = self.theta(self.x, alpha, beta)  \n",
    "        log_lik = np.sum(binom_dist.logpmf(self.y, n=self.N, p=theta), axis=-1) \n",
    "        \n",
    "        if type(log_lik) is np.ndarray:\n",
    "            log_lik = np.expand_dims(log_lik, axis=-1)\n",
    "        return log_lik\n",
    "\n",
    "    def log_joint(self, alpha, beta):\n",
    "        return self.log_prior(alpha, beta).squeeze() + self.log_likelihood(alpha, beta).squeeze()\n",
    "    \n",
    "# instantiate model\n",
    "model = LogisticRegression(x, y, N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
