{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c49859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# If you need to import from a local utils.py, uncomment and adjust the following lines:\n",
    "import sys\n",
    "import os\n",
    "import jax.numpy as jnp\n",
    "# Construct the full path to the folder\n",
    "folder_path = r'C:\\Users\\Petrb\\Desktop\\DTU\\3rdSemester\\02477_BAYESIAN_MACHINE_LEARNING'\n",
    "\n",
    "# Add the folder to the Python path\n",
    "sys.path.append(folder_path)\n",
    "\n",
    "# Now you can import the utils module\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552550a",
   "metadata": {},
   "source": [
    "## Part 1: Multi-class classification\n",
    "\n",
    "Consider the following linear model for multi-class classification with $ K = 3 $ classes:\n",
    "\n",
    "$$\n",
    "y_n | \\mathbf{f}_n \\sim \\text{Categorical}(\\text{softmax}(\\mathbf{f}_n)), \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{f}_n = \\mathbf{W} \\phi(x_n), \\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{ij} \\sim \\mathcal{N}(0, \\alpha^{-1}), \\tag{3}\n",
    "$$\n",
    "\n",
    "where $ y_n \\in \\{1,2,3\\} $, $ x_n \\in \\mathbb{R} $, $ \\alpha > 0 $ is a hyperparameter, and $ \\mathbf{W} $ are the parameters of interest. The feature transformation $ \\phi(x) $ is given by $ \\phi(x) = \\begin{bmatrix} 1 & x \\end{bmatrix}^T $ such that $ \\mathbf{W} \\in \\mathbb{R}^{K \\times D} $ for $ D = 2 $.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.1: Identify the prior and likelihood of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa8ca6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "222aeb00",
   "metadata": {},
   "source": [
    "Let\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{W}}_{\\text{MAP}} = \\begin{bmatrix}\n",
    "-0.5 & -2.0 \\\\\n",
    "3.0 & 0.0 \\\\\n",
    "1.0 & 1.0\n",
    "\\end{bmatrix} \\tag{4}\n",
    "$$\n",
    "\n",
    "be a MAP-estimator for the model given in eq. (1)â€“(3) for some dataset $ \\mathcal{D} $ (not given).\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.2: Use the plugin approximation with $ \\hat{\\mathbf{W}}_{\\text{MAP}} $ to compute the posterior predictive distribution for $ x^* = -1 $.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f559a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_MAP: \n",
      " [[-0.5 -2. ]\n",
      " [ 3.   0. ]\n",
      " [ 1.   1. ]]\n",
      "w_MAP shape: (3, 2)\n",
      "Pred for x_star: [1.5 3.  0. ]\n",
      "Pred for x_stars shape: (3,)\n",
      "Softmax: [0.17529039 0.785597   0.03911257]\n"
     ]
    }
   ],
   "source": [
    "w_MAP = jnp.array([[-0.5, -2], [3.0, 0.0], [1.0, 1.0]])\n",
    "print(f\"w_MAP: \\n {w_MAP}\")\n",
    "print(f\"w_MAP shape: {w_MAP.shape}\")\n",
    "\n",
    "x_star = -1.0\n",
    "\n",
    "def f(x, w):\n",
    "    \"\"\"\n",
    "    Function to compute the output of a linear model with weights w and input x.\n",
    "    :param x: Input data (2D array).\n",
    "    :param w: Weights (2D array).\n",
    "    :return: Output of the linear model (1D array).\n",
    "    \"\"\"\n",
    "    return jnp.dot(x, w.T)\n",
    "\n",
    "\n",
    "def phi(x):\n",
    "    return jnp.array([1, x])\n",
    "\n",
    "pred = w_MAP @ phi(x_star)  # Compute the prediction for x_star using the weights w_MAP\n",
    "#print(phi(x_star) @ w_MAP.T)\n",
    "print(f\"Pred for x_star: {pred}\")\n",
    "print(f\"Pred for x_stars shape: {pred.shape}\")\n",
    "\n",
    "\n",
    "softmax = lambda x: jnp.exp(x) / jnp.sum(jnp.exp(x)) \n",
    "\n",
    "print(f\"Softmax: {softmax(pred)}\")  \n",
    "\n",
    "#print(f\"Prob: {prob}\")\n",
    "#print(f\"Prob shape: {prob.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c085db8",
   "metadata": {},
   "source": [
    "Let $ \\mathbf{W}^{(i)} \\sim q(\\mathbf{W}) $ for $ i = 1, 2, 3 $ be samples from a variational approximation of the posterior, i.e. $ p(\\mathbf{W}|\\mathcal{D}) \\approx q(\\mathbf{W}) $:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{(1)} = \\begin{bmatrix}\n",
    "-0.15 & -1.92 \\\\\n",
    "3.2 & 0.45 \\\\\n",
    "1.37 & 0.8\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{W}^{(2)} = \\begin{bmatrix}\n",
    "-0.31 & -2.03 \\\\\n",
    "2.98 & 0.08 \\\\\n",
    "1.03 & 1.29\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{W}^{(3)} = \\begin{bmatrix}\n",
    "-0.35 & -1.98 \\\\\n",
    "3.09 & 0.07 \\\\\n",
    "1.3 & 0.96\n",
    "\\end{bmatrix}. \\tag{5}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Question 1.3: Compute a Monte Carlo estimate of the posterior predictive distribution for $ x^* = -1 $ using samples given above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc16b0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs: [0.2229536  0.72390676 0.05313967]\n"
     ]
    }
   ],
   "source": [
    "w_1 = jnp.array([[-0.15, -1.92], [3.2, 0.45], [1.37, 0.8]])\n",
    "w_2 = jnp.array([[-0.31, -2.03], [2.98, 0.08], [1.03, 1.29]])\n",
    "w_3 = jnp.array([[-0.35, -1.98], [3.09, 0.07], [1.3, 0.96]])\n",
    "\n",
    "\n",
    "list_of_weights = [w_1, w_2, w_3]\n",
    "\n",
    "def phi(x):\n",
    "    return jnp.array([1, x])\n",
    "\n",
    "\n",
    "probs = []\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    pred = list_of_weights[i] @ phi(x_star)  # Compute the prediction for x_star using the weights w_MAP\n",
    "    probs.append(softmax(pred))\n",
    "\n",
    "print(f\"Probs: {np.mean(probs, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b688fac",
   "metadata": {},
   "source": [
    "The predictive distribution $ p(y^* | \\mathcal{D}, x^* = 3) $ is given in the table below:\n",
    "\n",
    "| $ k $ | $ p(y^* = k \\mid x^*) $ |\n",
    "|--------|---------------------------|\n",
    "| 1      | 0.00                      |\n",
    "| 2      | 0.27                      |\n",
    "| 3      | 0.73                      |\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.4: Determine the entropy and confidence of the posterior predictive distribution for $ x^* = 3 $ given in the table above.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6cd1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(p):\n",
    "    \"\"\"\n",
    "    Computes the confidence for each predictive distribution.\n",
    "\n",
    "    The confidence is defined as the maximum predicted probability for each sample:\n",
    "        confidence(x^*) = max_k p(y^*=k | x^*, D)\n",
    "    where D is the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : jax.numpy.ndarray\n",
    "        Posterior predictive probabilities for each sample and class.\n",
    "        Shape: (N, K), where N is the number of prediction points and K is the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    conf : jax.numpy.ndarray\n",
    "        Confidence for each prediction point.\n",
    "        Shape: (N,)\n",
    "\n",
    "    Equation\n",
    "    --------\n",
    "    conf_n = max_k p_{n,k}\n",
    "    \"\"\"\n",
    "    return jnp.max(p, axis=1)\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"\n",
    "    Computes the predictive entropy for each predictive distribution.\n",
    "\n",
    "    The entropy measures the uncertainty of the predictive distribution:\n",
    "        entropy(x^*) = -sum_k p(y^*=k | x^*, D) * log(p(y^*=k | x^*, D))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : jax.numpy.ndarray\n",
    "        Posterior predictive probabilities for each sample and class.\n",
    "        Shape: (N, K), where N is the number of prediction points and K is the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ent : jax.numpy.ndarray\n",
    "        Predictive entropy for each prediction point.\n",
    "        Shape: (N,)\n",
    "\n",
    "    Equation\n",
    "    --------\n",
    "    ent_n = -sum_k p_{n,k} * log(p_{n,k})\n",
    "    \"\"\"\n",
    "    # Use jnp.where to avoid log(0) by only computing log where p > 0\n",
    "    return -jnp.sum(jnp.where(p > 0, p * jnp.log(p), 0.0), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "245e9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_hat: [[0.   0.27 0.73]]\n",
      "p_hat shape: (1, 3)\n",
      "Confidence: [0.73]\n",
      "Entropy: [0.58325887]\n"
     ]
    }
   ],
   "source": [
    "p_hat = jnp.array([0.0, 0.27, 0.73]).reshape(1, 3)  \n",
    "print(f\"p_hat: {p_hat}\")\n",
    "print(f\"p_hat shape: {p_hat.shape}\")\n",
    "\n",
    "\n",
    "confidence(p_hat)\n",
    "print(f\"Confidence: {confidence(p_hat)}\")\n",
    "entropy(p_hat)\n",
    "print(f\"Entropy: {entropy(p_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb3e65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 1.5: Suppose the value of the hyperparameter $ \\alpha $ is increased by a factor of 10. Explain in your own words how you would expect the MAP-estimate to change and why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02477_Bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
