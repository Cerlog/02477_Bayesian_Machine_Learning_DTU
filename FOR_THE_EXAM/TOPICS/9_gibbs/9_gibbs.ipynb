{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gamma, poisson\n",
    "import jax.random as random\n",
    "from jax import vmap, jit\n",
    "\n",
    "print(\"=== MCMC CONVERGENCE DIAGNOSTICS - COMPLETE IMPLEMENTATION ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: R-hat Calculation\n",
    "# =============================================================================\n",
    "print(\"STEP 1: R-hat (Potential Scale Reduction Factor)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def compute_Rhat(chains):\n",
    "    \"\"\"\n",
    "    Compute the potential scale reduction factor (R-hat) for MCMC chains.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    chains : array-like, shape (M, S, P)\n",
    "        M = number of chains\n",
    "        S = number of samples per chain\n",
    "        P = number of parameters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Rhat : array-like, shape (P,)\n",
    "        R-hat for each parameter\n",
    "    \"\"\"\n",
    "    M, S, P = chains.shape\n",
    "    \n",
    "    # Initialize array for R-hat values\n",
    "    Rhat = jnp.zeros(P)\n",
    "    \n",
    "    for p in range(P):\n",
    "        # Extract parameter p from all chains\n",
    "        param_chains = chains[:, :, p]\n",
    "        \n",
    "        # Compute chain means\n",
    "        chain_means = jnp.mean(param_chains, axis=1)\n",
    "        \n",
    "        # Compute overall mean\n",
    "        overall_mean = jnp.mean(param_chains)\n",
    "        \n",
    "        # Within-chain variance\n",
    "        W = jnp.mean(jnp.var(param_chains, axis=1, ddof=1))\n",
    "        \n",
    "        # Between-chain variance\n",
    "        B = S * jnp.var(chain_means, ddof=1)\n",
    "        \n",
    "        # Compute R-hat\n",
    "        R_squared = (S - 1) / S + B / (S * W)\n",
    "        Rhat = Rhat.at[p].set(jnp.sqrt(R_squared))\n",
    "    \n",
    "    return Rhat\n",
    "\n",
    "# Small hand calculation example\n",
    "print(\"Hand calculation example:\")\n",
    "chain1 = jnp.array([1.2, 1.4, 1.1, 1.3, 1.2])\n",
    "chain2 = jnp.array([1.8, 1.6, 1.9, 1.7, 1.5])\n",
    "chains_small = jnp.stack([chain1, chain2])[:, :, None]\n",
    "\n",
    "R_hat_small = compute_Rhat(chains_small)\n",
    "print(f\"Chains:\\n{chains_small[:, :, 0]}\")\n",
    "print(f\"R-hat = {R_hat_small[0]:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Effective Sample Size Calculation\n",
    "# =============================================================================\n",
    "print(\"\\n\\nSTEP 2: Effective Sample Size (ESS)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def compute_autocorrelation(x, max_lag=None):\n",
    "    \"\"\"Compute autocorrelation function up to max_lag\"\"\"\n",
    "    n = len(x)\n",
    "    if max_lag is None:\n",
    "        max_lag = n // 4\n",
    "    \n",
    "    x_centered = x - jnp.mean(x)\n",
    "    c0 = jnp.dot(x_centered, x_centered) / n\n",
    "    \n",
    "    autocorr = []\n",
    "    for lag in range(max_lag + 1):\n",
    "        if lag == 0:\n",
    "            autocorr.append(1.0)\n",
    "        else:\n",
    "            ck = jnp.dot(x_centered[:-lag], x_centered[lag:]) / (n - lag)\n",
    "            autocorr.append(ck / c0)\n",
    "    \n",
    "    return jnp.array(autocorr)\n",
    "\n",
    "def compute_effective_sample_size(chains):\n",
    "    \"\"\"\n",
    "    Compute effective sample size for MCMC chains.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    chains : array-like, shape (M, S, P)\n",
    "        M = number of chains\n",
    "        S = number of samples per chain\n",
    "        P = number of parameters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ESS : array-like, shape (P,)\n",
    "        Effective sample size for each parameter\n",
    "    \"\"\"\n",
    "    M, S, P = chains.shape\n",
    "    ESS = jnp.zeros(P)\n",
    "    \n",
    "    for p in range(P):\n",
    "        # Merge all chains for parameter p\n",
    "        merged_chain = chains[:, :, p].ravel()\n",
    "        n = len(merged_chain)\n",
    "        \n",
    "        # Compute autocorrelation\n",
    "        max_lag = min(n // 4, 1000)\n",
    "        autocorr = compute_autocorrelation(merged_chain, max_lag)\n",
    "        \n",
    "        # Find first negative autocorrelation\n",
    "        sum_autocorr = 1.0\n",
    "        for lag in range(1, len(autocorr)):\n",
    "            if autocorr[lag] < 0:\n",
    "                break\n",
    "            sum_autocorr += 2 * autocorr[lag]\n",
    "        \n",
    "        ESS = ESS.at[p].set(n / sum_autocorr)\n",
    "    \n",
    "    return ESS\n",
    "\n",
    "# Example calculation\n",
    "print(\"ESS calculation example:\")\n",
    "ess_small = compute_effective_sample_size(chains_small)\n",
    "print(f\"Total samples: {chains_small.size}\")\n",
    "print(f\"ESS = {ess_small[0]:.3f}\")\n",
    "print(f\"Efficiency = {ess_small[0]/chains_small.size:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: Multiple Metropolis Chains\n",
    "# =============================================================================\n",
    "print(\"\\n\\nSTEP 3: Multiple Metropolis Chains\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def metropolis(log_target, num_params, tau, num_iter, theta_init=None, seed=0):\n",
    "    \"\"\"Basic Metropolis algorithm with Gaussian proposals\"\"\"\n",
    "    if theta_init is None:\n",
    "        theta_init = jnp.zeros(num_params)\n",
    "    \n",
    "    # Initialize\n",
    "    key = random.PRNGKey(seed)\n",
    "    samples = jnp.zeros((num_iter + 1, num_params))\n",
    "    samples = samples.at[0].set(theta_init)\n",
    "    current_log_prob = log_target(theta_init)\n",
    "    \n",
    "    n_accepted = 0\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        key, subkey = random.split(key)\n",
    "        \n",
    "        # Propose new state\n",
    "        proposal = samples[i] + tau * random.normal(subkey, shape=(num_params,))\n",
    "        \n",
    "        # Compute acceptance ratio\n",
    "        proposal_log_prob = log_target(proposal)\n",
    "        log_ratio = proposal_log_prob - current_log_prob\n",
    "        \n",
    "        # Accept/reject\n",
    "        key, accept_key = random.split(key)\n",
    "        if jnp.log(random.uniform(accept_key)) < log_ratio:\n",
    "            samples = samples.at[i + 1].set(proposal)\n",
    "            current_log_prob = proposal_log_prob\n",
    "            n_accepted += 1\n",
    "        else:\n",
    "            samples = samples.at[i + 1].set(samples[i])\n",
    "    \n",
    "    accept_rate = n_accepted / num_iter\n",
    "    return samples, accept_rate\n",
    "\n",
    "def metropolis_multiple_chains(log_target, num_params, num_chains, tau, num_iter, \n",
    "                             theta_init, seeds, warm_up=0):\n",
    "    \"\"\"Run multiple Metropolis chains in parallel\"\"\"\n",
    "    # Prepare storage\n",
    "    thetas = []\n",
    "    accept_rates = []\n",
    "    \n",
    "    # Run each chain\n",
    "    for idx_chain in range(num_chains):\n",
    "        print(f'Running chain {idx_chain + 1}/{num_chains}')\n",
    "        samples, accept_rate = metropolis(log_target, num_params, tau, num_iter,\n",
    "                                        theta_init=theta_init[idx_chain], \n",
    "                                        seed=seeds[idx_chain])\n",
    "        thetas.append(samples)\n",
    "        accept_rates.append(accept_rate)\n",
    "    \n",
    "    # Stack chains\n",
    "    thetas = jnp.stack(thetas, axis=0)\n",
    "    accept_rates = jnp.array(accept_rates)\n",
    "    \n",
    "    # Discard warmup\n",
    "    thetas = thetas[:, warm_up:, :]\n",
    "    \n",
    "    return thetas, accept_rates\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Bimodal Distribution Example\n",
    "# =============================================================================\n",
    "print(\"\\n\\nSTEP 4: Bimodal Distribution Example\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Define bimodal distribution\n",
    "def log_bimodal(x):\n",
    "    \"\"\"Log density of mixture: 0.5 * N(-3, 4) + 0.5 * N(1, 2)\"\"\"\n",
    "    log_p1 = jnp.log(0.5) + norm.logpdf(x, loc=-3, scale=2)\n",
    "    log_p2 = jnp.log(0.5) + norm.logpdf(x, loc=1, scale=jnp.sqrt(2))\n",
    "    return jnp.logaddexp(log_p1, log_p2).sum()\n",
    "\n",
    "# True statistics\n",
    "true_mean = -1.0\n",
    "true_var = 7.0\n",
    "print(f\"True mean: {true_mean}\")\n",
    "print(f\"True variance: {true_var}\")\n",
    "\n",
    "# MCMC settings\n",
    "num_chains = 4\n",
    "num_iter = 1000\n",
    "proposal_variance = 0.1\n",
    "num_params = 1\n",
    "warm_up = 500\n",
    "\n",
    "# Initial values\n",
    "key = random.PRNGKey(1)\n",
    "theta_init = 5 * random.normal(key, shape=(num_chains, num_params))\n",
    "seeds = jnp.arange(num_chains)\n",
    "\n",
    "# Run chains\n",
    "print(\"\\nRunning MCMC for bimodal distribution...\")\n",
    "chains_bimodal, accepts = metropolis_multiple_chains(\n",
    "    log_bimodal, num_params, num_chains, proposal_variance, \n",
    "    num_iter, theta_init, seeds, warm_up)\n",
    "\n",
    "# Compute diagnostics\n",
    "Rhat_bimodal = compute_Rhat(chains_bimodal)\n",
    "ESS_bimodal = compute_effective_sample_size(chains_bimodal)\n",
    "\n",
    "# Results\n",
    "merged_samples = chains_bimodal.ravel()\n",
    "estimated_mean = jnp.mean(merged_samples)\n",
    "estimated_var = jnp.var(merged_samples)\n",
    "\n",
    "print(f\"\\nEstimated mean: {estimated_mean:.3f}\")\n",
    "print(f\"Estimated variance: {estimated_var:.3f}\")\n",
    "print(f\"R-hat: {Rhat_bimodal[0]:.3f}\")\n",
    "print(f\"ESS: {ESS_bimodal[0]:.0f}\")\n",
    "print(f\"Relative efficiency: {ESS_bimodal[0]/(num_chains*(num_iter-warm_up)):.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Trace plots\n",
    "ax = axes[0]\n",
    "for i in range(num_chains):\n",
    "    ax.plot(chains_bimodal[i, :, 0], alpha=0.7, label=f'Chain {i+1}')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Sample value')\n",
    "ax.set_title(f'Trace plots (R̂={Rhat_bimodal[0]:.3f})')\n",
    "ax.legend()\n",
    "\n",
    "# Histogram vs true density\n",
    "ax = axes[1]\n",
    "x_range = jnp.linspace(-8, 6, 1000)\n",
    "true_density = 0.5 * norm.pdf(x_range, -3, 2) + 0.5 * norm.pdf(x_range, 1, jnp.sqrt(2))\n",
    "ax.hist(merged_samples, bins=50, density=True, alpha=0.7, label='Samples')\n",
    "ax.plot(x_range, true_density, 'r-', linewidth=2, label='True density')\n",
    "ax.axvline(true_mean, color='g', linestyle='--', label='True mean')\n",
    "ax.axvline(estimated_mean, color='b', linestyle='--', label='Est. mean')\n",
    "ax.set_title('Posterior samples')\n",
    "ax.legend()\n",
    "\n",
    "# Autocorrelation\n",
    "ax = axes[2]\n",
    "autocorr = compute_autocorrelation(merged_samples, max_lag=50)\n",
    "ax.plot(autocorr, 'o-')\n",
    "ax.axhline(0, color='k', linestyle='--')\n",
    "ax.set_xlabel('Lag')\n",
    "ax.set_ylabel('Autocorrelation')\n",
    "ax.set_title(f'Autocorrelation (ESS={ESS_bimodal[0]:.0f})')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: Change Point Detection Model\n",
    "# =============================================================================\n",
    "print(\"\\n\\nSTEP 5: Change Point Detection Model\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Generate simulated data\n",
    "np.random.seed(42)\n",
    "N = 50\n",
    "true_c = 25\n",
    "true_lambda1 = 2.0\n",
    "true_lambda2 = 5.0\n",
    "\n",
    "accident_counts = jnp.concatenate([\n",
    "    jnp.array(np.random.poisson(true_lambda1, true_c)),\n",
    "    jnp.array(np.random.poisson(true_lambda2, N - true_c))\n",
    "])\n",
    "\n",
    "year = jnp.arange(1850, 1850 + N)\n",
    "\n",
    "print(f\"Simulated data: N={N}, true change point={true_c}\")\n",
    "print(f\"True λ₁={true_lambda1}, true λ₂={true_lambda2}\")\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.scatter(year, accident_counts, alpha=0.7)\n",
    "ax.axvline(year[true_c], color='r', linestyle='--', label='True change point')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Accident count')\n",
    "ax.set_title('Simulated Accident Data')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Gibbs sampler\n",
    "def cpd_gibbs_sampler(x, alpha, beta, num_iter, c_init, lambda1_init, \n",
    "                     lambda2_init, warmup=0, seed=0):\n",
    "    \"\"\"Gibbs sampler for change point detection\"\"\"\n",
    "    N = len(x)\n",
    "    \n",
    "    # Storage\n",
    "    lambda1_samples = [lambda1_init]\n",
    "    lambda2_samples = [lambda2_init]\n",
    "    c_samples = [c_init]\n",
    "    \n",
    "    key = random.PRNGKey(seed)\n",
    "    \n",
    "    for k in range(num_iter):\n",
    "        key, subkey = random.split(key)\n",
    "        key1, key2, key3 = random.split(subkey, num=3)\n",
    "        \n",
    "        # Sample λ₁\n",
    "        c_k = int(c_samples[k])\n",
    "        a1 = alpha + jnp.sum(x[:c_k])\n",
    "        b1 = 1.0 / (beta + c_k)\n",
    "        lambda1_new = b1 * random.gamma(key1, a1)\n",
    "        lambda1_samples.append(lambda1_new)\n",
    "        \n",
    "        # Sample λ₂\n",
    "        a2 = alpha + jnp.sum(x[c_k:])\n",
    "        b2 = 1.0 / (beta + N - c_k)\n",
    "        lambda2_new = b2 * random.gamma(key2, a2)\n",
    "        lambda2_samples.append(lambda2_new)\n",
    "        \n",
    "        # Sample c\n",
    "        log_prob_c = []\n",
    "        for ci in range(N):\n",
    "            log_p = (jnp.sum(x[:ci]) * jnp.log(lambda1_samples[k+1]) - \n",
    "                    ci * lambda1_samples[k+1] +\n",
    "                    jnp.sum(x[ci:]) * jnp.log(lambda2_samples[k+1]) - \n",
    "                    (N - ci) * lambda2_samples[k+1])\n",
    "            log_prob_c.append(log_p)\n",
    "        \n",
    "        log_prob_c = jnp.array(log_prob_c)\n",
    "        log_prob_c = log_prob_c - jnp.max(log_prob_c)\n",
    "        prob_c = jnp.exp(log_prob_c)\n",
    "        prob_c = prob_c / jnp.sum(prob_c)\n",
    "        \n",
    "        c_new = random.choice(key3, jnp.arange(N), p=prob_c)\n",
    "        c_samples.append(c_new)\n",
    "        \n",
    "        if (k + 1) % (num_iter // 5) == 0:\n",
    "            print(f'Iteration {k + 1}/{num_iter}')\n",
    "    \n",
    "    # Discard warmup\n",
    "    if warmup > 0:\n",
    "        lambda1_samples = lambda1_samples[warmup:]\n",
    "        lambda2_samples = lambda2_samples[warmup:]\n",
    "        c_samples = c_samples[warmup:]\n",
    "    \n",
    "    return (jnp.array(lambda1_samples), \n",
    "            jnp.array(lambda2_samples), \n",
    "            jnp.array(c_samples))\n",
    "\n",
    "# Run multiple chains\n",
    "print(\"\\nRunning Gibbs sampler with multiple chains...\")\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "num_iter = 2000\n",
    "num_chains = 4\n",
    "warmup = 1000\n",
    "\n",
    "# Storage for chains\n",
    "all_samples = []\n",
    "\n",
    "key = random.PRNGKey(1)\n",
    "for chain_idx in range(num_chains):\n",
    "    print(f'\\nChain {chain_idx + 1}/{num_chains}')\n",
    "    \n",
    "    # Random initial values\n",
    "    key, subkey = random.split(key)\n",
    "    key1, key2, key3 = random.split(subkey, num=3)\n",
    "    \n",
    "    c_init = random.choice(key1, jnp.arange(N))\n",
    "    l1_init = 1/beta * random.gamma(key2, alpha)\n",
    "    l2_init = 1/beta * random.gamma(key3, alpha)\n",
    "    \n",
    "    # Run sampler\n",
    "    l1, l2, c = cpd_gibbs_sampler(accident_counts, alpha, beta, num_iter,\n",
    "                                  c_init, l1_init, l2_init, warmup, seed=chain_idx)\n",
    "    \n",
    "    # Stack parameters\n",
    "    chain_samples = jnp.stack([c, l1, l2], axis=1)\n",
    "    all_samples.append(chain_samples)\n",
    "\n",
    "# Stack all chains\n",
    "all_samples = jnp.stack(all_samples, axis=0)\n",
    "\n",
    "# Compute diagnostics\n",
    "print(\"\\nComputing convergence diagnostics...\")\n",
    "Rhat_cpd = compute_Rhat(all_samples)\n",
    "ESS_cpd = compute_effective_sample_size(all_samples)\n",
    "\n",
    "parameter_names = ['c', 'λ₁', 'λ₂']\n",
    "print(\"\\nParameter | R-hat | ESS | Rel. Efficiency\")\n",
    "print(\"-\" * 40)\n",
    "for i, name in enumerate(parameter_names):\n",
    "    rel_eff = ESS_cpd[i] / (num_chains * (num_iter - warmup))\n",
    "    print(f\"{name:9s} | {Rhat_cpd[i]:.3f} | {ESS_cpd[i]:.0f} | {rel_eff:.3f}\")\n",
    "\n",
    "# Posterior analysis\n",
    "merged_samples = all_samples.reshape(-1, 3)\n",
    "c_samples = merged_samples[:, 0].astype(int)\n",
    "lambda1_samples = merged_samples[:, 1]\n",
    "lambda2_samples = merged_samples[:, 2]\n",
    "\n",
    "print(\"\\nPosterior means:\")\n",
    "print(f\"c: {jnp.mean(c_samples):.1f} (true: {true_c})\")\n",
    "print(f\"λ₁: {jnp.mean(lambda1_samples):.3f} (true: {true_lambda1})\")\n",
    "print(f\"λ₂: {jnp.mean(lambda2_samples):.3f} (true: {true_lambda2})\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Trace plots\n",
    "for i, (param_name, true_val) in enumerate(zip(parameter_names, \n",
    "                                               [true_c, true_lambda1, true_lambda2])):\n",
    "    ax = axes[0, i]\n",
    "    for chain_idx in range(num_chains):\n",
    "        ax.plot(all_samples[chain_idx, :, i], alpha=0.7, label=f'Chain {chain_idx+1}')\n",
    "    ax.axhline(true_val, color='r', linestyle='--', label='True value')\n",
    "    ax.set_title(f'{param_name} (R̂={Rhat_cpd[i]:.3f})')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel(param_name)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "# Posterior distributions\n",
    "for i, (param_name, samples, true_val) in enumerate(zip(\n",
    "    parameter_names, \n",
    "    [c_samples, lambda1_samples, lambda2_samples],\n",
    "    [true_c, true_lambda1, true_lambda2])):\n",
    "    \n",
    "    ax = axes[1, i]\n",
    "    ax.hist(samples, bins=30, density=True, alpha=0.7)\n",
    "    ax.axvline(true_val, color='r', linestyle='--', label='True value')\n",
    "    ax.axvline(jnp.mean(samples), color='g', linestyle='--', label='Posterior mean')\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_title(f'Posterior of {param_name}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: Advanced Analysis - Parameter Efficiency\n",
    "# =============================================================================\n",
    "print(\"\\n\\nSTEP 6: Parameter Efficiency Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sweep over proposal variances for bimodal distribution\n",
    "taus = jnp.logspace(-2, 2, 20)\n",
    "R_effs = []\n",
    "\n",
    "print(\"Testing different proposal variances...\")\n",
    "for idx_tau, tau in enumerate(taus):\n",
    "    print(f'Testing τ = {tau:.3f} ({idx_tau+1}/{len(taus)})')\n",
    "    \n",
    "    chains, _ = metropolis_multiple_chains(\n",
    "        log_bimodal, 1, 4, tau, 1000, \n",
    "        5*random.normal(random.PRNGKey(123), shape=(4, 1)), \n",
    "        jnp.arange(4), warm_up=500)\n",
    "    \n",
    "    ESS = compute_effective_sample_size(chains)\n",
    "    total_samples = chains.size\n",
    "    R_effs.append(ESS[0] / total_samples)\n",
    "\n",
    "R_effs = jnp.array(R_effs)\n",
    "idx_optimal = jnp.argmax(R_effs)\n",
    "\n",
    "print(f\"\\nOptimal proposal variance: {taus[idx_optimal]:.3f}\")\n",
    "print(f\"Maximum efficiency: {R_effs[idx_optimal]:.3f}\")\n",
    "\n",
    "# Plot efficiency curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.semilogx(taus, R_effs, 'o-')\n",
    "ax.axvline(taus[idx_optimal], color='r', linestyle='--', \n",
    "          label=f'Optimal τ = {taus[idx_optimal]:.3f}')\n",
    "ax.set_xlabel('Proposal variance τ')\n",
    "ax.set_ylabel('Relative efficiency')\n",
    "ax.set_title('Efficiency vs Proposal Variance')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\\n=== SUMMARY ===\")\n",
    "print(\"-\" * 50)\n",
    "print(\"MCMC Convergence Diagnostics:\")\n",
    "print(\"1. R-hat measures convergence by comparing within/between chain variance\")\n",
    "print(\"2. ESS accounts for autocorrelation to give effective sample size\")\n",
    "print(\"3. Multiple chains help diagnose convergence issues\")\n",
    "print(\"\\nKey findings:\")\n",
    "print(f\"- Bimodal distribution: R̂={Rhat_bimodal[0]:.3f}, ESS={ESS_bimodal[0]:.0f}\")\n",
    "print(f\"- Change point model: R̂={Rhat_cpd.mean():.3f}, ESS={ESS_cpd.mean():.0f}\")\n",
    "print(\"- Optimal proposal variance maximizes efficiency\")\n",
    "print(\"- Gibbs sampling often more efficient than Metropolis for structured models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ce7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
