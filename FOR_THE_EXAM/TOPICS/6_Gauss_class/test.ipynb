{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bec26b6",
   "metadata": {},
   "source": [
    "# Gaussian Process Classification: Complete Theory and Implementation Guide\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction and Purpose](#introduction)\n",
    "2. [Mathematical Foundation](#mathematical-foundation)\n",
    "3. [Prior and Likelihood Definition](#prior-likelihood)\n",
    "4. [Posterior Approximation via Laplace Method](#posterior-approximation)\n",
    "5. [Posterior Predictive Distribution](#predictive-distribution)\n",
    "6. [Implementation Details](#implementation)\n",
    "7. [Hand Calculations Example](#hand-calculations)\n",
    "8. [Making Predictions](#predictions)\n",
    "9. [Code Walkthrough](#code-walkthrough)\n",
    "\n",
    "<a name=\"introduction\"></a>\n",
    "## 1. Introduction and Purpose\n",
    "\n",
    "Gaussian Process Classification (GPC) is a probabilistic, non-parametric approach to binary classification that:\n",
    "\n",
    "- **Solves**: Binary classification problems where we need to distinguish between two classes\n",
    "- **Provides**: Probabilistic predictions with uncertainty quantification\n",
    "- **Excels at**: Learning complex, non-linear decision boundaries without specifying a fixed functional form\n",
    "- **Use cases**: Medical diagnosis, image classification, anomaly detection\n",
    "\n",
    "### Key Advantages:\n",
    "- Automatic complexity control (no overfitting)\n",
    "- Uncertainty estimates for predictions\n",
    "- Flexible, non-parametric model\n",
    "- Principled Bayesian framework\n",
    "\n",
    "<a name=\"mathematical-foundation\"></a>\n",
    "## 2. Mathematical Foundation\n",
    "\n",
    "The GPC model is defined hierarchically:\n",
    "\n",
    "$$\\begin{align}\n",
    "y|f(\\mathbf{x}) &\\sim \\text{Bernoulli}[\\sigma(f(\\mathbf{x}))] \\\\\n",
    "f(\\mathbf{x}) &\\sim \\mathcal{GP}(0, k(\\mathbf{x}, \\mathbf{x}'))\n",
    "\\end{align}$$\n",
    "\n",
    "Where:\n",
    "- $f(\\mathbf{x})$ is a latent function following a Gaussian Process\n",
    "- $\\sigma(\\cdot)$ is the sigmoid function: $\\sigma(f) = \\frac{1}{1 + e^{-f}}$\n",
    "- $k(\\mathbf{x}, \\mathbf{x}')$ is the covariance (kernel) function\n",
    "\n",
    "<a name=\"prior-likelihood\"></a>\n",
    "## 3. Prior and Likelihood Definition\n",
    "\n",
    "### 3.1 Prior Distribution\n",
    "\n",
    "The prior over the latent function is a Gaussian Process:\n",
    "\n",
    "$$p(\\mathbf{f}) = \\mathcal{N}(\\mathbf{f}|\\mathbf{0}, \\mathbf{K})$$\n",
    "\n",
    "Where $\\mathbf{K}_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$ is the kernel matrix.\n",
    "\n",
    "**Squared Exponential Kernel:**\n",
    "$$k(\\mathbf{x}, \\mathbf{x}') = \\kappa \\exp\\left(-\\frac{||\\mathbf{x} - \\mathbf{x}'||^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "Parameters:\n",
    "- $\\kappa$: magnitude (signal variance)\n",
    "- $\\ell$: lengthscale (smoothness)\n",
    "\n",
    "### 3.2 Likelihood\n",
    "\n",
    "For binary classification with labels $y \\in \\{0, 1\\}$:\n",
    "\n",
    "$$p(y_n|f_n) = \\text{Bernoulli}(y_n|\\sigma(f_n)) = \\sigma(f_n)^{y_n}(1-\\sigma(f_n))^{1-y_n}$$\n",
    "\n",
    "Log-likelihood:\n",
    "$$\\log p(y_n|f_n) = y_n \\log \\sigma(f_n) + (1-y_n) \\log(1-\\sigma(f_n))$$\n",
    "\n",
    "### 3.3 Joint Distribution\n",
    "\n",
    "$$p(\\mathbf{y}, \\mathbf{f}) = p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f}) = \\prod_{n=1}^N p(y_n|f_n) \\cdot \\mathcal{N}(\\mathbf{f}|\\mathbf{0}, \\mathbf{K})$$\n",
    "\n",
    "<a name=\"posterior-approximation\"></a>\n",
    "## 4. Posterior Approximation via Laplace Method\n",
    "\n",
    "Since $p(\\mathbf{f}|\\mathbf{y})$ is intractable, we use the Laplace approximation:\n",
    "\n",
    "$$p(\\mathbf{f}|\\mathbf{y}) \\approx q(\\mathbf{f}) = \\mathcal{N}(\\mathbf{f}|\\mathbf{m}, \\mathbf{S})$$\n",
    "\n",
    "### 4.1 Finding the MAP Estimate\n",
    "\n",
    "First, find $\\mathbf{f}_{\\text{MAP}} = \\arg\\max_{\\mathbf{f}} \\log p(\\mathbf{y}, \\mathbf{f})$\n",
    "\n",
    "$$\\log p(\\mathbf{y}, \\mathbf{f}) = \\sum_{n=1}^N \\log p(y_n|f_n) - \\frac{1}{2}\\mathbf{f}^T\\mathbf{K}^{-1}\\mathbf{f} - \\frac{1}{2}\\log|\\mathbf{K}| - \\frac{N}{2}\\log(2\\pi)$$\n",
    "\n",
    "**Gradient:**\n",
    "$$\\nabla_{\\mathbf{f}} \\log p(\\mathbf{y}, \\mathbf{f}) = \\mathbf{y} - \\boldsymbol{\\sigma}(\\mathbf{f}) - \\mathbf{K}^{-1}\\mathbf{f}$$\n",
    "\n",
    "Where $\\boldsymbol{\\sigma}(\\mathbf{f}) = [\\sigma(f_1), ..., \\sigma(f_N)]^T$\n",
    "\n",
    "**Hessian:**\n",
    "$$\\nabla_{\\mathbf{f}}^2 \\log p(\\mathbf{y}, \\mathbf{f}) = -\\boldsymbol{\\Lambda} - \\mathbf{K}^{-1}$$\n",
    "\n",
    "Where $\\boldsymbol{\\Lambda} = \\text{diag}(\\sigma(f_n)(1-\\sigma(f_n)))$\n",
    "\n",
    "### 4.2 Laplace Approximation Parameters\n",
    "\n",
    "- Mean: $\\mathbf{m} = \\mathbf{f}_{\\text{MAP}}$\n",
    "- Covariance: $\\mathbf{S} = (\\mathbf{K}^{-1} + \\boldsymbol{\\Lambda})^{-1}$\n",
    "\n",
    "### 4.3 Numerically Stable Computation\n",
    "\n",
    "Using Woodbury identity to avoid direct inversion:\n",
    "\n",
    "$$\\mathbf{S} = \\mathbf{K} - \\mathbf{K}\\boldsymbol{\\Lambda}^{1/2}(\\mathbf{I} + \\boldsymbol{\\Lambda}^{1/2}\\mathbf{K}\\boldsymbol{\\Lambda}^{1/2})^{-1}\\boldsymbol{\\Lambda}^{1/2}\\mathbf{K}$$\n",
    "\n",
    "Algorithm:\n",
    "1. Compute $\\boldsymbol{\\Lambda}^{1/2} = \\text{diag}(\\sqrt{\\sigma(f_n)(1-\\sigma(f_n))})$\n",
    "2. Form $\\mathbf{B} = \\mathbf{I} + \\boldsymbol{\\Lambda}^{1/2}\\mathbf{K}\\boldsymbol{\\Lambda}^{1/2}$\n",
    "3. Compute Cholesky: $\\mathbf{B} = \\mathbf{L}_B\\mathbf{L}_B^T$\n",
    "4. Solve: $\\mathbf{e} = \\mathbf{L}_B^{-1}(\\boldsymbol{\\Lambda}^{1/2}\\mathbf{K})$\n",
    "5. Compute: $\\mathbf{S} = \\mathbf{K} - \\mathbf{e}^T\\mathbf{e}$\n",
    "\n",
    "<a name=\"predictive-distribution\"></a>\n",
    "## 5. Posterior Predictive Distribution\n",
    "\n",
    "### 5.1 Predictive Distribution for Latent Function\n",
    "\n",
    "For new input $\\mathbf{x}_*$:\n",
    "\n",
    "$$p(f_*|\\mathbf{y}, \\mathbf{x}_*) \\approx \\mathcal{N}(f_*|\\mu_*, \\sigma_*^2)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_* = \\mathbf{k}_*^T \\mathbf{K}^{-1} \\mathbf{m}$\n",
    "- $\\sigma_*^2 = k_{**} - \\mathbf{k}_*^T \\mathbf{K}^{-1} (\\mathbf{K} - \\mathbf{S}) \\mathbf{K}^{-1} \\mathbf{k}_*$\n",
    "\n",
    "With:\n",
    "- $\\mathbf{k}_* = [k(\\mathbf{x}_*, \\mathbf{x}_1), ..., k(\\mathbf{x}_*, \\mathbf{x}_N)]^T$\n",
    "- $k_{**} = k(\\mathbf{x}_*, \\mathbf{x}_*)$\n",
    "\n",
    "### 5.2 Predictive Probability for Class Label\n",
    "\n",
    "$$p(y_*=1|\\mathbf{y}, \\mathbf{x}_*) = \\int \\sigma(f_*) p(f_*|\\mathbf{y}, \\mathbf{x}_*) df_*$$\n",
    "\n",
    "Using probit approximation:\n",
    "\n",
    "$$p(y_*=1|\\mathbf{y}, \\mathbf{x}_*) \\approx \\Phi\\left(\\frac{\\mu_*}{\\sqrt{\\frac{8}{\\pi} + \\sigma_*^2}}\\right)$$\n",
    "\n",
    "Where $\\Phi$ is the standard normal CDF.\n",
    "\n",
    "<a name=\"implementation\"></a>\n",
    "## 6. Implementation Details\n",
    "\n",
    "### 6.1 Key Classes\n",
    "\n",
    "```python\n",
    "class BernoulliLikelihood:\n",
    "    \"\"\"Implements Bernoulli likelihood with sigmoid link\"\"\"\n",
    "    \n",
    "    def log_lik(self, f):\n",
    "        # log p(y|f) = y·log(σ(f)) + (1-y)·log(1-σ(f))\n",
    "        return jnp.sum(self.y * jnp.log(sigmoid(f)) + \n",
    "                      (1 - self.y) * jnp.log(1 - sigmoid(f)))\n",
    "    \n",
    "    def grad(self, f):\n",
    "        # ∇f log p(y|f) = y - σ(f)\n",
    "        return self.y - sigmoid(f)\n",
    "    \n",
    "    def hessian(self, f):\n",
    "        # ∇²f log p(y|f) = -diag(σ(f)(1-σ(f)))\n",
    "        return jnp.diag(-sigmoid(f) * (1 - sigmoid(f)))\n",
    "```\n",
    "\n",
    "```python\n",
    "class GaussianProcessClassification:\n",
    "    \"\"\"Main GPC class using Laplace approximation\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, likelihood, kernel, kappa, lengthscale):\n",
    "        # Store data and parameters\n",
    "        self.X, self.y = X, y\n",
    "        self.likelihood = likelihood(y)\n",
    "        self.kernel = kernel\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        self.K = kernel.construct_kernel(X, X)\n",
    "        self.L = jnp.linalg.cholesky(self.K)\n",
    "        \n",
    "        # Construct Laplace approximation\n",
    "        self.construct_laplace_approximation()\n",
    "```\n",
    "\n",
    "### 6.2 MAP Optimization\n",
    "\n",
    "The code uses a reparameterization $\\mathbf{f} = \\mathbf{K}\\mathbf{a}$ to avoid direct inversion:\n",
    "\n",
    "```python\n",
    "def log_joint_a(self, a):\n",
    "    f = self.K @ a  # Reparameterization\n",
    "    log_prior = -0.5 * jnp.sum(a * f) - jnp.sum(jnp.log(jnp.diag(self.L)))\n",
    "    log_lik = self.likelihood.log_lik(f)\n",
    "    return log_prior + log_lik\n",
    "\n",
    "def compute_f_MAP(self):\n",
    "    result = minimize(lambda a: -self.log_joint_a(a),\n",
    "                     jac=lambda a: -self.grad_a(a),\n",
    "                     x0=jnp.zeros(self.N))\n",
    "    return self.K @ result.x\n",
    "```\n",
    "\n",
    "<a name=\"hand-calculations\"></a>\n",
    "## 7. Hand Calculations Example\n",
    "\n",
    "Let's work through a small example with N=3 training points:\n",
    "\n",
    "### Setup\n",
    "- Training inputs: $\\mathbf{X} = [0, 1, 2]^T$\n",
    "- Training labels: $\\mathbf{y} = [0, 1, 0]^T$\n",
    "- Kernel parameters: $\\kappa = 1, \\ell = 1$\n",
    "\n",
    "### Step 1: Compute Kernel Matrix\n",
    "\n",
    "$$K_{ij} = \\exp\\left(-\\frac{(x_i - x_j)^2}{2}\\right)$$\n",
    "\n",
    "$$\\mathbf{K} = \\begin{bmatrix}\n",
    "1.000 & 0.607 & 0.135 \\\\\n",
    "0.607 & 1.000 & 0.607 \\\\\n",
    "0.135 & 0.607 & 1.000\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Step 2: Find MAP Estimate\n",
    "\n",
    "Initialize: $\\mathbf{f}^{(0)} = [0, 0, 0]^T$\n",
    "\n",
    "Iterate using gradient ascent:\n",
    "$$\\mathbf{f}^{(t+1)} = \\mathbf{f}^{(t)} + \\alpha \\nabla \\log p(\\mathbf{y}, \\mathbf{f}^{(t)})$$\n",
    "\n",
    "At convergence: $\\mathbf{f}_{\\text{MAP}} \\approx [-0.8, 0.9, -0.8]^T$\n",
    "\n",
    "### Step 3: Compute Hessian\n",
    "\n",
    "At MAP:\n",
    "- $\\sigma(f_1) \\approx 0.31, \\sigma(f_2) \\approx 0.71, \\sigma(f_3) \\approx 0.31$\n",
    "- $\\Lambda_{11} = 0.31 \\times 0.69 \\approx 0.214$\n",
    "- $\\Lambda_{22} = 0.71 \\times 0.29 \\approx 0.206$\n",
    "- $\\Lambda_{33} = 0.31 \\times 0.69 \\approx 0.214$\n",
    "\n",
    "$$\\boldsymbol{\\Lambda} = \\begin{bmatrix}\n",
    "0.214 & 0 & 0 \\\\\n",
    "0 & 0.206 & 0 \\\\\n",
    "0 & 0 & 0.214\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Step 4: Compute Posterior Covariance\n",
    "\n",
    "Using the stable algorithm:\n",
    "1. $\\boldsymbol{\\Lambda}^{1/2} = \\text{diag}(0.463, 0.454, 0.463)$\n",
    "2. $\\mathbf{B} = \\mathbf{I} + \\boldsymbol{\\Lambda}^{1/2}\\mathbf{K}\\boldsymbol{\\Lambda}^{1/2}$\n",
    "3. Compute $\\mathbf{S}$ using Cholesky decomposition\n",
    "\n",
    "### Step 5: Prediction at $x_* = 1.5$\n",
    "\n",
    "1. Compute $\\mathbf{k}_* = [k(1.5, 0), k(1.5, 1), k(1.5, 2)]^T$\n",
    "   $$\\mathbf{k}_* = [0.325, 0.607, 0.607]^T$$\n",
    "\n",
    "2. Compute predictive mean:\n",
    "   $$\\mu_* = \\mathbf{k}_*^T \\mathbf{K}^{-1} \\mathbf{m} \\approx 0.3$$\n",
    "\n",
    "3. Compute predictive variance:\n",
    "   $$\\sigma_*^2 = k_{**} - \\mathbf{k}_*^T \\mathbf{K}^{-1}(\\mathbf{K} - \\mathbf{S})\\mathbf{K}^{-1}\\mathbf{k}_* \\approx 0.15$$\n",
    "\n",
    "4. Compute class probability:\n",
    "   $$p(y_*=1) \\approx \\Phi\\left(\\frac{0.3}{\\sqrt{2.546 + 0.15}}\\right) \\approx 0.53$$\n",
    "\n",
    "<a name=\"predictions\"></a>\n",
    "## 8. Making Predictions\n",
    "\n",
    "### 8.1 Point Predictions\n",
    "\n",
    "```python\n",
    "# Predict probabilities\n",
    "p_test = gpc.predict_y(Xtest)\n",
    "\n",
    "# Binary classification\n",
    "ytest_hat = (p_test > 0.5).astype(float)\n",
    "```\n",
    "\n",
    "### 8.2 Uncertainty Quantification\n",
    "\n",
    "```python\n",
    "# Get predictive distribution\n",
    "mu, Sigma = gpc.predict_f(Xstar)\n",
    "\n",
    "# Predictive mean and variance\n",
    "mean_predictions = mu\n",
    "variance_predictions = jnp.diag(Sigma)\n",
    "\n",
    "# 95% confidence intervals\n",
    "lower_bound = mu - 1.96 * jnp.sqrt(jnp.diag(Sigma))\n",
    "upper_bound = mu + 1.96 * jnp.sqrt(jnp.diag(Sigma))\n",
    "```\n",
    "\n",
    "### 8.3 Sampling from Posterior\n",
    "\n",
    "```python\n",
    "# Generate samples from posterior predictive\n",
    "f_samples = gpc.posterior_samples(Xstar, num_samples=100)\n",
    "\n",
    "# Convert to probability samples\n",
    "p_samples = sigmoid(f_samples)\n",
    "```\n",
    "\n",
    "<a name=\"code-walkthrough\"></a>\n",
    "## 9. Code Walkthrough\n",
    "\n",
    "### 9.1 Complete Training Pipeline\n",
    "\n",
    "```python\n",
    "# 1. Setup kernel and likelihood\n",
    "kernel = StationaryIsotropicKernel(squared_exponential)\n",
    "likelihood = BernoulliLikelihood\n",
    "\n",
    "# 2. Create and train GPC model\n",
    "gpc = GaussianProcessClassification(\n",
    "    X, y, \n",
    "    likelihood, \n",
    "    kernel, \n",
    "    kappa=3.0,      # Signal variance\n",
    "    lengthscale=1.0  # Smoothness parameter\n",
    ")\n",
    "\n",
    "# 3. Make predictions\n",
    "mu, Sigma = gpc.predict_f(Xtest)  # Latent function\n",
    "p_test = gpc.predict_y(Xtest)     # Class probabilities\n",
    "```\n",
    "\n",
    "### 9.2 Visualization Example\n",
    "\n",
    "```python\n",
    "# Create prediction grid\n",
    "x_grid = np.linspace(-5, 5, 50)\n",
    "X_grid = np.meshgrid(x_grid, x_grid)\n",
    "X_flat = np.column_stack([X_grid[0].ravel(), X_grid[1].ravel()])\n",
    "\n",
    "# Predict on grid\n",
    "p_grid = gpc.predict_y(X_flat).reshape(50, 50)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(X_grid[0], X_grid[1], p_grid, levels=20, cmap='RdBu')\n",
    "plt.colorbar(label='P(y=1)')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "plt.title('GPC Decision Boundary')\n",
    "```\n",
    "\n",
    "### 9.3 Model Selection\n",
    "\n",
    "```python\n",
    "# Grid search over hyperparameters\n",
    "kappa_values = [0.1, 1.0, 10.0]\n",
    "lengthscale_values = [0.1, 1.0, 10.0]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for kappa in kappa_values:\n",
    "    for lengthscale in lengthscale_values:\n",
    "        gpc = GaussianProcessClassification(\n",
    "            X_train, y_train, \n",
    "            likelihood, kernel,\n",
    "            kappa=kappa, \n",
    "            lengthscale=lengthscale\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        p_val = gpc.predict_y(X_val)\n",
    "        log_likelihood = np.sum(\n",
    "            y_val * np.log(p_val) + \n",
    "            (1 - y_val) * np.log(1 - p_val)\n",
    "        )\n",
    "        \n",
    "        if log_likelihood > best_score:\n",
    "            best_score = log_likelihood\n",
    "            best_params = (kappa, lengthscale)\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "Gaussian Process Classification provides a principled, probabilistic approach to binary classification that:\n",
    "\n",
    "1. **Models complexity automatically** through the kernel function\n",
    "2. **Provides uncertainty estimates** via the posterior predictive distribution\n",
    "3. **Handles non-linear boundaries** without explicit feature engineering\n",
    "4. **Scales reasonably** to moderate-sized datasets (up to ~10,000 points)\n",
    "\n",
    "The Laplace approximation makes inference tractable while maintaining the benefits of the Bayesian framework, providing both point predictions and principled uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e216fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
